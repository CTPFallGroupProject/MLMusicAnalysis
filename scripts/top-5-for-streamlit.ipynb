{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qii68AeEKI5Q"
   },
   "source": [
    "# Using TF-IDF & cosine similarity to build a lyrically similar song search engine\n",
    "\n",
    "based off this article https://alliescomputing.com/knowledge-base/christmas-carol-search-using-tf-idf-and-cosine-similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GHcU_nwZKI5T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksandrageorgievska/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksandrageorgievska/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aleksandrageorgievska/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "#for top-5-similar songs recommender\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "#for text preprocessing:\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eC2dpzVKI5V"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3ybSaZYxKYOl"
   },
   "outputs": [],
   "source": [
    "# using the preprocessed lyrics dataset \n",
    "df = pd.read_csv('../data/preprocessed_dataset.csv')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Recommender Algorithm:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHFS4SQsKI5Z"
   },
   "source": [
    "## Determine the term frequencies (TFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a CountVectorizer to learn the terms and term frequencies across all of the documents (carols) \n",
    "cv = CountVectorizer() #type is CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 5 seconds\n",
    "\n",
    "doc_term_matrix = cv.fit_transform(df['clean_lyrics']) #type is csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the inverse document frequencies (IDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the term frequencies, now determine the inverse document frequencies (IDFs)\n",
    "idfs = TfidfTransformer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs.fit(doc_term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfTransformer"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame with the IDF values \n",
    "idfs_df = pd.DataFrame(idfs.idf_, index=cv.get_feature_names(), columns=[\"idfs\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together to calculate the TF-IDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO SAVE THIS FILE\n",
    "# check the type\n",
    "# check how it can be saved - what format?\n",
    "\n",
    "# We have the term frequencies and inverse document frequencies - now calculate the TF-IDF scores\n",
    "tf_idfs = idfs.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save tf_idfs & load\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(\"tf_idfs_top5.npz\", tf_idfs)\n",
    "# tf_idfs_top5 = sparse.load_npz(\"tf_idfs_top5.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now prepare a search query from user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user input must be preprocessed before feeding into cv.transform([query])\n",
    "\n",
    "# 1. function that makes all text lowercase.\n",
    "def make_lowercase(test_string):\n",
    "    return test_string.lower()\n",
    "\n",
    "# 2. function that removes all punctuation. \n",
    "def remove_punc(test_string):\n",
    "    test_string = re.sub(r'[^\\w\\s]', '', test_string)\n",
    "    return test_string\n",
    "\n",
    "# 3. function that removes all stopwords.\n",
    "def remove_stopwords(test_string):\n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(test_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "    \n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        \n",
    "        # Check if word is not in stopwords. Stopwords was imported from nltk.corpus\n",
    "        if word not in stopwords:\n",
    "            \n",
    "            # If word not in stopwords, append to our valid_words\n",
    "            valid_words.append(word)\n",
    "\n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string\n",
    "\n",
    "# 4. function to break words into their stem words\n",
    "def stem_words(a_string):\n",
    "    # Initalize our Stemmer\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(a_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "\n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        # Stem the word\n",
    "        stemmed_word = porter.stem(word) #from nltk.stem import PorterStemmer\n",
    "        \n",
    "        # Append stemmed word to our valid_words\n",
    "        valid_words.append(stemmed_word)\n",
    "        \n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline function \n",
    "\n",
    "def text_processing_pipeline(a_string):\n",
    "    a_string = make_lowercase(a_string)\n",
    "    a_string = remove_punc(a_string)\n",
    "    #a_string = stem_words(a_string) #removing stem_words for now because making lyrics gibberish\n",
    "    a_string = remove_stopwords(a_string)\n",
    "    return a_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your lyrics: # I love you baby and if its quite alright i need you baby\n"
     ]
    }
   ],
   "source": [
    "#get user input for lyrics\n",
    "\n",
    "query = input(\"Enter your lyrics: \") \n",
    "# I love you baby and if its quite alright i need you baby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using user input \n",
    "\n",
    "#query = text_processing_pipeline(query) #use this function call for cleaning data in THIS notebook \n",
    "\n",
    "# query = cleaning_data.clean_data(query) #use this function call for streamlit app \n",
    "\n",
    "# Calculate term frequencies for the query using terms found across all of the documents\n",
    "query_term_matrix = cv.transform([query]) #using user input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Across all of the terms, view the word counts for the query\n",
    "query_counts = pd.DataFrame(query_term_matrix.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the cosine similarity between the TF-IDFs and the query words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       ],\n",
       "       [0.       ],\n",
       "       [0.0030632],\n",
       "       ...,\n",
       "       [0.0211686],\n",
       "       [0.       ],\n",
       "       [0.       ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the cosine similarity between the vector of each document and the query vector\n",
    "results = cosine_similarity(tf_idfs, query_term_matrix)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.       , 0.       , 0.0030632, ..., 0.0211686, 0.       ,\n",
       "       0.       ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.reshape((-1,))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for input: \n",
      " \n",
      "# I love you baby and if its quite alright i need you baby\n",
      "\n",
      "Top 5 most similar songs based on lyrics are: \n",
      "\n",
      "- Straight into a Storm by Deer Tick at index 80596 with 73% match\n",
      "- A  for Andrew by Attack Attack! at index 139998 with 71% match\n",
      "- Too Nice to Talk To by The English Beat at index 136880 with 67% match\n",
      "- Turn It On by Lindsey Buckingham at index 141961 with 65% match\n",
      "- Lesson Number One by Marshall Crenshaw at index 55783 with 65% match\n"
     ]
    }
   ],
   "source": [
    "# argsort sorts an array in asc order, and then returns the indexes of the sorted values\n",
    "# Useful slice notation reference: https://stackoverflow.com/questions/509211/understanding-slice-notation \n",
    "# [:-6:-1] returns the last 5 items, in reverse order\n",
    "print(\"Search results for input: \\n \")\n",
    "print(\"{}\".format(query))\n",
    "print(\"\\nTop 5 most similar songs based on lyrics are: \\n\")\n",
    "\n",
    "for i in results.argsort()[:-6:-1]:\n",
    "    if results[i] > 0:\n",
    "        print(\"- {} at index {} with {}% match\".format(df.loc[i].song_by_artist, df.iloc[i,0], round(100*results[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
