{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qii68AeEKI5Q"
   },
   "source": [
    "# Using TF-IDF & cosine similarity to build a lyrically similar song search engine\n",
    "\n",
    "based off this article https://alliescomputing.com/knowledge-base/christmas-carol-search-using-tf-idf-and-cosine-similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GHcU_nwZKI5T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksandrageorgievska/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksandrageorgievska/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aleksandrageorgievska/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "#for top-5-similar songs recommender\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "#for text preprocessing:\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eC2dpzVKI5V"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3ybSaZYxKYOl"
   },
   "outputs": [],
   "source": [
    "# using the preprocessed lyrics dataset \n",
    "df = pd.read_csv('../data/preprocessed_dataset.csv')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Recommender Algorithm:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHFS4SQsKI5Z"
   },
   "source": [
    "## Determine the term frequencies (TFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a CountVectorizer to learn the terms and term frequencies across all of the documents (carols) \n",
    "cv = CountVectorizer() #type is CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 5 seconds\n",
    "\n",
    "doc_term_matrix = cv.fit_transform(df['clean_lyrics']) #type is csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the inverse document frequencies (IDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the term frequencies, now determine the inverse document frequencies (IDFs)\n",
    "idfs = TfidfTransformer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs.fit(doc_term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame with the IDF values \n",
    "idfs_df = pd.DataFrame(idfs.idf_, index=cv.get_feature_names(), columns=[\"idfs\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together to calculate the TF-IDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO SAVE THIS FILE\n",
    "# check the type\n",
    "# check how it can be saved - what format?\n",
    "\n",
    "# We have the term frequencies and inverse document frequencies - now calculate the TF-IDF scores\n",
    "tf_idfs = idfs.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102282, 142030)\n"
     ]
    }
   ],
   "source": [
    "print(tf_idfs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 141110)\t0.009121546266179486\n",
      "  (0, 140201)\t0.03397468322051298\n",
      "  (0, 139526)\t0.014194719423345735\n",
      "  (0, 138284)\t0.03237323842608716\n",
      "  (0, 135918)\t0.015426310702033156\n",
      "  (0, 135852)\t0.018658440076217534\n",
      "  (0, 135576)\t0.034562265226991254\n",
      "  (0, 127805)\t0.024128533175101396\n",
      "  (0, 124936)\t0.015572320891999412\n",
      "  (0, 124624)\t0.07433340262533938\n",
      "  (0, 124313)\t0.1883373156559461\n",
      "  (0, 122505)\t0.06171480830492754\n",
      "  (0, 122157)\t0.014416817248640676\n",
      "  (0, 121626)\t0.04107866693588441\n",
      "  (0, 119208)\t0.020238522707922632\n",
      "  (0, 118202)\t0.013597721696570458\n",
      "  (0, 118094)\t0.03478587953198509\n",
      "  (0, 118058)\t0.02987132692807049\n",
      "  (0, 116639)\t0.0580185739958205\n",
      "  (0, 116556)\t0.10809481003467643\n",
      "  (0, 116098)\t0.01631305384816871\n",
      "  (0, 112793)\t0.031932665927804645\n",
      "  (0, 112742)\t0.016787353369106008\n",
      "  (0, 111814)\t0.021743824854119492\n",
      "  (0, 111355)\t0.049274609986886486\n",
      "  :\t:\n",
      "  (102281, 65889)\t0.09197557615347597\n",
      "  (102281, 58322)\t0.06215473624740694\n",
      "  (102281, 56009)\t0.1289076173307143\n",
      "  (102281, 51937)\t0.06747255080363158\n",
      "  (102281, 50703)\t0.03460006171685029\n",
      "  (102281, 45459)\t0.07361456525727803\n",
      "  (102281, 44478)\t0.12717259069277856\n",
      "  (102281, 44246)\t0.07974668051896924\n",
      "  (102281, 43684)\t0.08238295801588205\n",
      "  (102281, 40478)\t0.08085352906377934\n",
      "  (102281, 37358)\t0.055144201731119905\n",
      "  (102281, 32873)\t0.22962027345073602\n",
      "  (102281, 31310)\t0.05578005563073841\n",
      "  (102281, 29899)\t0.08665853867394004\n",
      "  (102281, 26345)\t0.1758593545305598\n",
      "  (102281, 26344)\t0.3360092824271158\n",
      "  (102281, 25167)\t0.09087678695830663\n",
      "  (102281, 23795)\t0.09496414673333843\n",
      "  (102281, 23393)\t0.10276404087026401\n",
      "  (102281, 17739)\t0.06274874551502188\n",
      "  (102281, 17732)\t0.07290136167568381\n",
      "  (102281, 17463)\t0.11644096237998298\n",
      "  (102281, 10359)\t0.057994057341846546\n",
      "  (102281, 5385)\t0.09134952633452936\n",
      "  (102281, 2714)\t0.14045364081531417\n"
     ]
    }
   ],
   "source": [
    "print(tf_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save tf_idfs & load\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(\"tf_idfs_top5.npz\", tf_idfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idfs_top5 = sparse.load_npz(\"tf_idfs_top5.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102282, 142030)\n"
     ]
    }
   ],
   "source": [
    "print(tf_idfs_top5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 141110)\t0.009121546266179486\n",
      "  (0, 140201)\t0.03397468322051298\n",
      "  (0, 139526)\t0.014194719423345735\n",
      "  (0, 138284)\t0.03237323842608716\n",
      "  (0, 135918)\t0.015426310702033156\n",
      "  (0, 135852)\t0.018658440076217534\n",
      "  (0, 135576)\t0.034562265226991254\n",
      "  (0, 127805)\t0.024128533175101396\n",
      "  (0, 124936)\t0.015572320891999412\n",
      "  (0, 124624)\t0.07433340262533938\n",
      "  (0, 124313)\t0.1883373156559461\n",
      "  (0, 122505)\t0.06171480830492754\n",
      "  (0, 122157)\t0.014416817248640676\n",
      "  (0, 121626)\t0.04107866693588441\n",
      "  (0, 119208)\t0.020238522707922632\n",
      "  (0, 118202)\t0.013597721696570458\n",
      "  (0, 118094)\t0.03478587953198509\n",
      "  (0, 118058)\t0.02987132692807049\n",
      "  (0, 116639)\t0.0580185739958205\n",
      "  (0, 116556)\t0.10809481003467643\n",
      "  (0, 116098)\t0.01631305384816871\n",
      "  (0, 112793)\t0.031932665927804645\n",
      "  (0, 112742)\t0.016787353369106008\n",
      "  (0, 111814)\t0.021743824854119492\n",
      "  (0, 111355)\t0.049274609986886486\n",
      "  :\t:\n",
      "  (102281, 65889)\t0.09197557615347597\n",
      "  (102281, 58322)\t0.06215473624740694\n",
      "  (102281, 56009)\t0.1289076173307143\n",
      "  (102281, 51937)\t0.06747255080363158\n",
      "  (102281, 50703)\t0.03460006171685029\n",
      "  (102281, 45459)\t0.07361456525727803\n",
      "  (102281, 44478)\t0.12717259069277856\n",
      "  (102281, 44246)\t0.07974668051896924\n",
      "  (102281, 43684)\t0.08238295801588205\n",
      "  (102281, 40478)\t0.08085352906377934\n",
      "  (102281, 37358)\t0.055144201731119905\n",
      "  (102281, 32873)\t0.22962027345073602\n",
      "  (102281, 31310)\t0.05578005563073841\n",
      "  (102281, 29899)\t0.08665853867394004\n",
      "  (102281, 26345)\t0.1758593545305598\n",
      "  (102281, 26344)\t0.3360092824271158\n",
      "  (102281, 25167)\t0.09087678695830663\n",
      "  (102281, 23795)\t0.09496414673333843\n",
      "  (102281, 23393)\t0.10276404087026401\n",
      "  (102281, 17739)\t0.06274874551502188\n",
      "  (102281, 17732)\t0.07290136167568381\n",
      "  (102281, 17463)\t0.11644096237998298\n",
      "  (102281, 10359)\t0.057994057341846546\n",
      "  (102281, 5385)\t0.09134952633452936\n",
      "  (102281, 2714)\t0.14045364081531417\n"
     ]
    }
   ],
   "source": [
    "print(tf_idfs_top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now prepare a search query from user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user input must be preprocessed before feeding into cv.transform([query])\n",
    "\n",
    "# 1. function that makes all text lowercase.\n",
    "def make_lowercase(test_string):\n",
    "    return test_string.lower()\n",
    "\n",
    "# 2. function that removes all punctuation. \n",
    "def remove_punc(test_string):\n",
    "    test_string = re.sub(r'[^\\w\\s]', '', test_string)\n",
    "    return test_string\n",
    "\n",
    "# 3. function that removes all stopwords.\n",
    "def remove_stopwords(test_string):\n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(test_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "    \n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        \n",
    "        # Check if word is not in stopwords. Stopwords was imported from nltk.corpus\n",
    "        if word not in stopwords:\n",
    "            \n",
    "            # If word not in stopwords, append to our valid_words\n",
    "            valid_words.append(word)\n",
    "\n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string\n",
    "\n",
    "# 4. function to break words into their stem words\n",
    "def stem_words(a_string):\n",
    "    # Initalize our Stemmer\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(a_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "\n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        # Stem the word\n",
    "        stemmed_word = porter.stem(word) #from nltk.stem import PorterStemmer\n",
    "        \n",
    "        # Append stemmed word to our valid_words\n",
    "        valid_words.append(stemmed_word)\n",
    "        \n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline function \n",
    "\n",
    "def text_processing_pipeline(a_string):\n",
    "    a_string = make_lowercase(a_string)\n",
    "    a_string = remove_punc(a_string)\n",
    "    #a_string = stem_words(a_string) #removing stem_words for now because making lyrics gibberish\n",
    "    a_string = remove_stopwords(a_string)\n",
    "    return a_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user input for lyrics\n",
    "\n",
    "query = \"I love you baby and if it's quite alright I need you baby to warm the lonley night\"\n",
    "# I love you baby and if its quite alright i need you baby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using user input \n",
    "\n",
    "#query = text_processing_pipeline(query) #use this function call for cleaning data in THIS notebook \n",
    "\n",
    "# query = cleaning_data.clean_data(query) #use this function call for streamlit app \n",
    "\n",
    "# Calculate term frequencies for the query using terms found across all of the documents\n",
    "query_term_matrix = cv.transform([query]) #using user input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Across all of the terms, view the word counts for the query\n",
    "#             query_counts = pd.DataFrame(query_term_matrix.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the cosine similarity between the TF-IDFs and the query words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.00354109],\n",
       "       [0.0026123 ],\n",
       "       ...,\n",
       "       [0.01805265],\n",
       "       [0.        ],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the cosine similarity between the vector of each document and the query vector\n",
    "results = cosine_similarity(tf_idfs_top5, query_term_matrix)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00354109, 0.0026123 , ..., 0.01805265, 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.reshape((-1,))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for input: \n",
      " \n",
      "I love you baby and if it's quite alright I need you baby to warm the lonley night\n",
      "\n",
      "Top 5 most similar songs based on lyrics are: \n",
      "\n",
      "- Straight into a Storm by Deer Tick at index 80596 with 62% match\n",
      "- A  for Andrew by Attack Attack! at index 139998 with 61% match\n",
      "- Audacity of Huge by Simian Mobile Disco at index 142999 with 59% match\n",
      "- Too Nice to Talk To by The English Beat at index 136880 with 57% match\n",
      "- Give out, But Don't Give Up by The Supremes at index 89739 with 56% match\n"
     ]
    }
   ],
   "source": [
    "# argsort sorts an array in asc order, and then returns the indexes of the sorted values\n",
    "# Useful slice notation reference: https://stackoverflow.com/questions/509211/understanding-slice-notation \n",
    "# [:-6:-1] returns the last 5 items, in reverse order\n",
    "print(\"Search results for input: \\n \")\n",
    "print(\"{}\".format(query))\n",
    "print(\"\\nTop 5 most similar songs based on lyrics are: \\n\")\n",
    "\n",
    "for i in results.argsort()[:-6:-1]:\n",
    "    if results[i] > 0:\n",
    "        print(\"- {} at index {} with {}% match\".format(df.loc[i].song_by_artist, df.iloc[i,0], round(100*results[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
