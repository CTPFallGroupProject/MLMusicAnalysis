{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3020c80",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6a9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers, models, losses, optimizers,callbacks\n",
    "\n",
    "from keras.layers import Dense,LSTM\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f890099",
   "metadata": {},
   "source": [
    "# load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e137d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158353, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22651b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_lyrics'] = df['clean_lyrics'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0188c",
   "metadata": {},
   "source": [
    "# Create X and y feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6859345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "X = df['clean_lyrics']\n",
    "\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2316e3a",
   "metadata": {},
   "source": [
    "# Vectorize X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5975eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158353, 300)\n"
     ]
    }
   ],
   "source": [
    "# Limiting our tokenizers vocab size\n",
    "max_words = 10000\n",
    " \n",
    "    \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "\n",
    "# Fit the tokenizer\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "\n",
    "# Create the sequences for each sentence, basically turning each word into its index position\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "\n",
    "index_word = tokenizer.index_word\n",
    "\n",
    "\n",
    "# # Limiting our sequencer to only include 300 words\n",
    "max_length = 300\n",
    "\n",
    "\n",
    "# # Convert the sequences to all be the same length of 300\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ce2b7",
   "metadata": {},
   "source": [
    "# Split train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "772d6c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((126682, 300), (126682,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc713a46",
   "metadata": {},
   "source": [
    "# Building LSTM nueral net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78548331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 300, 32)           320000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 300, 50)           16600     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356,851\n",
      "Trainable params: 356,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This creates the Neural Network\n",
    "model = Sequential() \n",
    "\n",
    "# This embedding layer basically will automatically create the word2vec vectors based on your text data.\n",
    "model.add( Embedding(max_words, 32, input_length=max_length) ) \n",
    "\n",
    "model.add(LSTM(50,return_sequences=True,dropout =0.2))\n",
    "model.add(LSTM(50,dropout =0.2))\n",
    "model.add(Dense(1,kernel_initializer='normal',  activation='linear'))\n",
    "optimizer = optimizers.Adam(lr=0.003)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse']) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e9581",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0db4cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5068/5068 [==============================] - 495s 97ms/step - loss: 0.0608 - mse: 0.0608 - val_loss: 0.0541 - val_mse: 0.0541\n",
      "Epoch 2/15\n",
      "5068/5068 [==============================] - 491s 97ms/step - loss: 0.0521 - mse: 0.0521 - val_loss: 0.0516 - val_mse: 0.0516\n",
      "Epoch 3/15\n",
      "5068/5068 [==============================] - 491s 97ms/step - loss: 0.0493 - mse: 0.0493 - val_loss: 0.0507 - val_mse: 0.0507\n",
      "Epoch 4/15\n",
      "5068/5068 [==============================] - 501s 99ms/step - loss: 0.0474 - mse: 0.0474 - val_loss: 0.0505 - val_mse: 0.0505\n",
      "Epoch 5/15\n",
      "5068/5068 [==============================] - 511s 101ms/step - loss: 0.0458 - mse: 0.0458 - val_loss: 0.0509 - val_mse: 0.0509\n",
      "Epoch 6/15\n",
      "5068/5068 [==============================] - 519s 102ms/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.0507 - val_mse: 0.0507\n"
     ]
    }
   ],
   "source": [
    "callback = callbacks.EarlyStopping(monitor='val_mse',patience = 2,restore_best_weights=True)\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 validation_split=0.2,\n",
    "                 epochs=15, batch_size=20,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cdd989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_Valence_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4bb704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('LSTM_Valence_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ec2fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x23fdb4a6e20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b3a59",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bf0bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mse with stacked LSTM: 0.04978620260953903\n"
     ]
    }
   ],
   "source": [
    "mse= model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "\n",
    "print('Test mse with stacked LSTM:', mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
